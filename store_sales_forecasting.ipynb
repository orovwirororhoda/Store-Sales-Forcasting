{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè™ Store Sales Time Series Forecasting\n",
        "### ECON 5140: Applied Econometrics ‚Äî Midterm Project | Spring 2025\n",
        "\n",
        "> **Dataset:** [Kaggle Store Sales Competition](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)  \n",
        "> **Models:** SARIMA ¬∑ ETS (Holt-Winters) ¬∑ Prophet  \n",
        "> **Forecasting scope:** Aggregate sales + Product family level (GROCERY I, BEVERAGES)  \n",
        "> **Horizon:** 30 days ahead\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Setup\n",
        "\n",
        "Run the cell below to install dependencies if needed. Prophet requires a separate install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcf424df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install pandas numpy matplotlib seaborn statsmodels scikit-learn\n",
        "# !pip install prophet\n",
        "# !pip install kaggle  # for downloading the dataset\n",
        "\n",
        "# Download Kaggle data (after setting up kaggle.json credentials):\n",
        "# !kaggle competitions download -c store-sales-time-series-forecasting -p ./data\n",
        "# !cd data && unzip store-sales-time-series-forecasting.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e1a7bf",
      "metadata": {},
      "source": [
        "## üì¶ 0. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "334666b0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nECON 5140: Applied Econometrics ‚Äî Midterm Project\\nProject Option 3: Store Sales Time Series Forecasting\\n\\nAuthors: [Your Group Names]\\nDate: Spring 2025\\n\\nThis script covers:\\n  1. Data Loading & Preprocessing\\n  2. Exploratory Data Analysis (EDA)\\n  3. Feature Engineering\\n  4. Forecasting Models: ARIMA, ETS, Prophet\\n  5. Evaluation: MAE, RMSE, MAPE\\n  6. Visualizations & Confidence Intervals\\n  7. Both aggregate and product-family-level forecasts\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "ECON 5140: Applied Econometrics ‚Äî Midterm Project\n",
        "Project Option 3: Store Sales Time Series Forecasting\n",
        "\n",
        "Authors: [Your Group Names]\n",
        "Date: Spring 2025\n",
        "\n",
        "This script covers:\n",
        "  1. Data Loading & Preprocessing\n",
        "  2. Exploratory Data Analysis (EDA)\n",
        "  3. Feature Engineering\n",
        "  4. Forecasting Models: ARIMA, ETS, Prophet\n",
        "  5. Evaluation: MAE, RMSE, MAPE\n",
        "  6. Visualizations & Confidence Intervals\n",
        "  7. Both aggregate and product-family-level forecasts\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a5584c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ 0. SETUP & IMPORTS\n",
        "\n",
        "Import all required libraries and configure plot styling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c3af4eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# Statsmodels (ARIMA, ETS)\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "    PROPHET_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Prophet not installed. Install: pip install prophet --break-system-packages\")\n",
        "    PROPHET_AVAILABLE = False\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Plot style\n",
        "plt.rcParams.update({\n",
        "    \"figure.facecolor\": \"white\",\n",
        "    \"axes.facecolor\": \"white\",\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.alpha\": 0.3,\n",
        "    \"font.size\": 11,\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.spines.right\": False\n",
        "})\n",
        "PALETTE = [\"#1C7293\", \"#F96167\", \"#28A745\", \"#F4A261\", \"#6D2E46\"]\n",
        "\n",
        "# Output folder for plots\n",
        "PLOTS_DIR = Path(\"plots\")\n",
        "PLOTS_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07cda8b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìÇ 1. DATA LOADING\n",
        "\n",
        "Two loaders are provided:\n",
        "- **`load_data()`** ‚Äî reads real Kaggle CSV files from `./data/`\n",
        "- **`generate_synthetic_data()`** ‚Äî creates realistic demo data when Kaggle files aren't available yet\n",
        "\n",
        "The `main()` function automatically picks the right one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68386d93",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    \"\"\"\n",
        "    Load Kaggle Store Sales competition data.\n",
        "\n",
        "    Data files (download from Kaggle):\n",
        "      train.csv        - daily sales by store/family\n",
        "      test.csv         - prediction horizon\n",
        "      stores.csv       - store metadata\n",
        "      oil.csv          - daily oil price (macroeconomic indicator)\n",
        "      holidays_events.csv - Ecuador public holidays\n",
        "\n",
        "    Download instructions:\n",
        "      kaggle competitions download -c store-sales-time-series-forecasting -p ./data\n",
        "      cd ./data && unzip store-sales-time-series-forecasting.zip\n",
        "    \"\"\"\n",
        "    data_dir = Path(data_dir)\n",
        "\n",
        "    print(\"Loading data files...\")\n",
        "    train   = pd.read_csv(data_dir / \"train.csv\",            parse_dates=[\"date\"])\n",
        "    test    = pd.read_csv(data_dir / \"test.csv\",             parse_dates=[\"date\"])\n",
        "    stores  = pd.read_csv(data_dir / \"stores.csv\")\n",
        "    oil     = pd.read_csv(data_dir / \"oil.csv\",              parse_dates=[\"date\"])\n",
        "    hol     = pd.read_csv(data_dir / \"holidays_events.csv\",  parse_dates=[\"date\"])\n",
        "\n",
        "    print(f\"  Train: {train.shape}  |  Test: {test.shape}\")\n",
        "    print(f\"  Date range: {train['date'].min().date()} ‚Üí {train['date'].max().date()}\")\n",
        "    print(f\"  Stores: {train['store_nbr'].nunique()}  |  Families: {train['family'].nunique()}\")\n",
        "    return train, test, stores, oil, hol\n",
        "\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generate realistic synthetic data for demonstration when Kaggle data\n",
        "    is not yet downloaded. Mirrors the structure of the actual dataset.\n",
        "    \"\"\"\n",
        "    print(\"Generating synthetic demonstration data...\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    dates  = pd.date_range(\"2013-01-01\", \"2017-08-15\", freq=\"D\")\n",
        "    stores = list(range(1, 55))\n",
        "    families = [\n",
        "        \"GROCERY I\", \"BEVERAGES\", \"PRODUCE\", \"CLEANING\",\n",
        "        \"DAIRY\", \"BREAD/BAKERY\", \"POULTRY\", \"MEATS\",\n",
        "        \"PERSONAL CARE\", \"DELI\"\n",
        "    ]\n",
        "\n",
        "    rows = []\n",
        "    for store in stores:\n",
        "        for family in families:\n",
        "            base     = np.random.uniform(200, 2000)\n",
        "            trend    = np.linspace(0, base * 0.2, len(dates))\n",
        "            seasonal = base * 0.3 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\n",
        "            weekly   = base * 0.15 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)\n",
        "            noise    = np.random.normal(0, base * 0.05, len(dates))\n",
        "            sales    = np.maximum(0, base + trend + seasonal + weekly + noise)\n",
        "            for i, d in enumerate(dates):\n",
        "                rows.append({\"date\": d, \"store_nbr\": store,\n",
        "                             \"family\": family, \"sales\": sales[i],\n",
        "                             \"onpromotion\": int(np.random.random() < 0.1)})\n",
        "\n",
        "    train = pd.DataFrame(rows)\n",
        "\n",
        "    # Test: last 16 days\n",
        "    test_dates = pd.date_range(\"2017-08-16\", periods=16)\n",
        "    test_rows  = []\n",
        "    for store in stores:\n",
        "        for family in families:\n",
        "            for d in test_dates:\n",
        "                test_rows.append({\"date\": d, \"store_nbr\": store, \"family\": family, \"onpromotion\": 0})\n",
        "    test = pd.DataFrame(test_rows)\n",
        "\n",
        "    stores_df = pd.DataFrame({\n",
        "        \"store_nbr\": stores,\n",
        "        \"city\": np.random.choice([\"Quito\",\"Guayaquil\",\"Cuenca\",\"Ambato\"], 54),\n",
        "        \"state\": np.random.choice([\"Pichincha\",\"Guayas\",\"Azuay\"], 54),\n",
        "        \"type\": np.random.choice([\"A\",\"B\",\"C\",\"D\",\"E\"], 54),\n",
        "        \"cluster\": np.random.randint(1, 18, 54)\n",
        "    })\n",
        "\n",
        "    oil = pd.DataFrame({\n",
        "        \"date\": pd.date_range(\"2013-01-01\", \"2017-08-31\"),\n",
        "        \"dcoilwtico\": 90 + np.cumsum(np.random.normal(0, 1.5, len(pd.date_range(\"2013-01-01\",\"2017-08-31\"))))\n",
        "    })\n",
        "\n",
        "    hol = pd.DataFrame({\n",
        "        \"date\": pd.to_datetime([\"2013-01-01\",\"2013-04-01\",\"2013-12-25\",\n",
        "                                \"2014-01-01\",\"2014-04-18\",\"2014-12-25\",\n",
        "                                \"2015-01-01\",\"2015-04-03\",\"2015-12-25\",\n",
        "                                \"2016-01-01\",\"2016-03-25\",\"2016-12-25\",\n",
        "                                \"2017-01-01\",\"2017-04-14\",\"2017-12-25\"]),\n",
        "        \"type\": \"Holiday\",\n",
        "        \"locale\": \"National\",\n",
        "        \"locale_name\": \"Ecuador\",\n",
        "        \"description\": [\"New Year\",\"Easter\",\"Christmas\"] * 5,\n",
        "        \"transferred\": False\n",
        "    })\n",
        "\n",
        "    print(f\"  Synthetic Train: {train.shape}\")\n",
        "    print(f\"  Date range: {train['date'].min().date()} ‚Üí {train['date'].max().date()}\")\n",
        "    return train, test, stores_df, oil, hol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44302dbd",
      "metadata": {},
      "source": [
        "### Load data & preprocess (run to get train, test, df)\n",
        "\n",
        "Run this cell to load (or generate) data and preprocess. Creates `train`, `test`, `stores`, `oil`, `hol`, and `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e9c043",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data (use Kaggle files if present, else synthetic)\n",
        "try:\n",
        "    _ = train\n",
        "except NameError:\n",
        "    if Path(\"./data/train.csv\").exists():\n",
        "        train, test, stores, oil, hol = load_data(\"./data\")\n",
        "    else:\n",
        "        train, test, stores, oil, hol = generate_synthetic_data()\n",
        "    df = preprocess(train, stores, oil, hol)\n",
        "    print(f\"Train: {train.shape}  |  Test: {test.shape}  |  Preprocessed df: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6cd704",
      "metadata": {},
      "source": [
        "### Test set ‚Äî last 16 days\n",
        "\n",
        "Summary and preview of the test set (forecast horizon)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984602d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure train, test, df exist (run \"Load data & preprocess\" if needed)\n",
        "try:\n",
        "    _ = test\n",
        "except NameError:\n",
        "    if Path(\"./data/train.csv\").exists():\n",
        "        train, test, stores, oil, hol = load_data(\"./data\")\n",
        "    else:\n",
        "        train, test, stores, oil, hol = generate_synthetic_data()\n",
        "    df = preprocess(train, stores, oil, hol)\n",
        "\n",
        "print(\"Test set (last 16 days):\")\n",
        "print(f\"  Shape: {test.shape}\")\n",
        "print(f\"  Date range: {test['date'].min().date()} ‚Üí {test['date'].max().date()}\")\n",
        "print(f\"  Unique dates: {test['date'].nunique()}\")\n",
        "display(test.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cb849d",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß 2. PREPROCESSING\n",
        "\n",
        "Merge auxiliary datasets (stores, oil prices, holidays) onto the training data.  \n",
        "Create time features and clean zero/negative sales values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cdc416de",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(train, stores, oil, hol):\n",
        "    \"\"\"Merge auxiliary data, handle missing values, create features.\"\"\"\n",
        "\n",
        "    # Merge store metadata\n",
        "    df = train.merge(stores, on=\"store_nbr\", how=\"left\")\n",
        "\n",
        "    # Merge oil prices (forward-fill missing weekends)\n",
        "    oil_full = oil.set_index(\"date\").reindex(\n",
        "        pd.date_range(oil[\"date\"].min(), oil[\"date\"].max(), freq=\"D\")\n",
        "    ).ffill().reset_index().rename(columns={\"index\": \"date\"})\n",
        "    df = df.merge(oil_full, on=\"date\", how=\"left\")\n",
        "\n",
        "    # Merge holidays\n",
        "    hol_national = hol[hol[\"locale\"] == \"National\"][[\"date\",\"type\",\"description\"]].copy()\n",
        "    hol_national.columns = [\"date\",\"holiday_type\",\"holiday_desc\"]\n",
        "    df = df.merge(hol_national, on=\"date\", how=\"left\")\n",
        "    df[\"is_holiday\"] = df[\"holiday_type\"].notna().astype(int)\n",
        "\n",
        "    # Time features\n",
        "    df[\"year\"]       = df[\"date\"].dt.year\n",
        "    df[\"month\"]      = df[\"date\"].dt.month\n",
        "    df[\"day\"]        = df[\"date\"].dt.day\n",
        "    df[\"dayofweek\"]  = df[\"date\"].dt.dayofweek\n",
        "    df[\"weekofyear\"] = df[\"date\"].dt.isocalendar().week.astype(int)\n",
        "    df[\"quarter\"]    = df[\"date\"].dt.quarter\n",
        "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
        "\n",
        "    # Handle zeros (genuine closed days vs missing)\n",
        "    df[\"sales\"] = df[\"sales\"].clip(lower=0)\n",
        "\n",
        "    print(f\"Preprocessed DataFrame shape: {df.shape}\")\n",
        "    print(f\"  Missing oil prices: {df['dcoilwtico'].isna().sum()}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39155a8f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä 3. AGGREGATE TIME SERIES\n",
        "\n",
        "Build two series structures:\n",
        "- **Aggregate**: sum of all stores √ó all families ‚Üí single daily total\n",
        "- **Family-level**: separate series per product family (top N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "76560efc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_aggregate_series(df):\n",
        "    \"\"\"Aggregate all stores √ó families ‚Üí single daily total sales series.\"\"\"\n",
        "    agg = df.groupby(\"date\")[\"sales\"].sum().reset_index()\n",
        "    agg = agg.sort_values(\"date\").set_index(\"date\")\n",
        "    try:\n",
        "        agg.index.freq = \"D\"\n",
        "    except (AttributeError, ValueError):\n",
        "        pass\n",
        "    print(f\"Aggregate series: {len(agg)} daily observations\")\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f39bbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_family_series(df, top_n=5):\n",
        "    \"\"\"Aggregate by product family; return dict of top N families.\"\"\"\n",
        "    family_totals = df.groupby(\"family\")[\"sales\"].sum().sort_values(ascending=False)\n",
        "    top_families  = family_totals.head(top_n).index.tolist()\n",
        "    print(f\"Top {top_n} families: {top_families}\")\n",
        "\n",
        "    series_dict = {}\n",
        "    for fam in top_families:\n",
        "        s = df[df[\"family\"] == fam].groupby(\"date\")[\"sales\"].sum().sort_values()\n",
        "        s.index = pd.DatetimeIndex(s.index)\n",
        "        try:\n",
        "            s.index.freq = \"D\"\n",
        "        except (AttributeError, ValueError):\n",
        "            pass\n",
        "        series_dict[fam] = s\n",
        "    return series_dict, top_families"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e0e050",
      "metadata": {},
      "source": [
        "### Build time series from preprocessed data (run after Load data & preprocess)\n",
        "\n",
        "Creates `agg`, `family_dict`, and `top_families`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f778d493",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    _ = df\n",
        "except NameError:\n",
        "    if Path(\"./data/train.csv\").exists():\n",
        "        train, test, stores, oil, hol = load_data(\"./data\")\n",
        "    else:\n",
        "        train, test, stores, oil, hol = generate_synthetic_data()\n",
        "    df = preprocess(train, stores, oil, hol)\n",
        "agg = build_aggregate_series(df)\n",
        "family_dict, top_families = build_family_series(df, top_n=3)\n",
        "print(f\"Ready: agg with {len(agg)} days, {len(top_families)} families: {top_families}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206e404c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç 4. EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "Six EDA visualizations:\n",
        "1. Total daily sales over time\n",
        "2. Sales by product family (stacked subplots)\n",
        "3. Seasonal decomposition (trend, seasonal, residual)\n",
        "4. Day-of-week distribution (box plots)\n",
        "5. Monthly average sales (bar chart)\n",
        "6. ACF / PACF ‚Äî autocorrelation structure\n",
        "\n",
        "Also performs ADF stationarity test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ef10cf89",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eda_aggregate(agg, family_dict, top_families):\n",
        "    \"\"\"Comprehensive EDA plots.\"\"\"\n",
        "\n",
        "    # --- 4.1 Total Sales Over Time ---\n",
        "    fig, ax = plt.subplots(figsize=(14, 4))\n",
        "    ax.plot(agg.index, agg[\"sales\"] / 1e6, color=PALETTE[0], lw=1.2)\n",
        "    ax.set(title=\"Total Daily Store Sales (All Families, All Stores)\",\n",
        "           xlabel=\"Date\", ylabel=\"Sales (Millions)\")\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"01_aggregate_sales.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Plot saved: 01_aggregate_sales.png\")\n",
        "\n",
        "    # --- 4.2 Sales by Product Family ---\n",
        "    fig, axes = plt.subplots(len(top_families), 1, figsize=(14, 3*len(top_families)), sharex=True)\n",
        "    for ax, fam, color in zip(axes, top_families, PALETTE):\n",
        "        s = family_dict[fam]\n",
        "        ax.fill_between(s.index, s.values, alpha=0.3, color=color)\n",
        "        ax.plot(s.index, s.values, color=color, lw=1)\n",
        "        ax.set_ylabel(fam, fontsize=9)\n",
        "    axes[-1].set_xlabel(\"Date\")\n",
        "    fig.suptitle(\"Daily Sales by Top Product Family\", fontsize=13, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"02_family_sales.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4.3 Seasonal Decomposition ---\n",
        "    decomp = seasonal_decompose(agg[\"sales\"].dropna(), model=\"additive\", period=365)\n",
        "    fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
        "    titles = [\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"]\n",
        "    components = [decomp.observed, decomp.trend, decomp.seasonal, decomp.resid]\n",
        "    for ax, comp, title in zip(axes, components, titles):\n",
        "        ax.plot(comp.index, comp.values, color=PALETTE[0], lw=1)\n",
        "        ax.set_title(title, fontsize=10)\n",
        "    fig.suptitle(\"Seasonal Decomposition ‚Äî Aggregate Sales (Period=365)\", fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"03_decomposition.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4.4 Weekly seasonality box plot ---\n",
        "    tmp = agg.copy().reset_index()\n",
        "    tmp[\"day_name\"] = pd.to_datetime(tmp[\"date\"]).dt.day_name()\n",
        "    day_order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    tmp[\"day_name\"] = pd.Categorical(tmp[\"day_name\"], categories=day_order, ordered=True)\n",
        "    tmp.boxplot(column=\"sales\", by=\"day_name\", ax=ax, patch_artist=True)\n",
        "    ax.set(title=\"Sales Distribution by Day of Week\",\n",
        "           xlabel=\"Day\", ylabel=\"Daily Sales\")\n",
        "    plt.suptitle(\"\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"04_weekly_pattern.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4.5 Monthly pattern ---\n",
        "    tmp[\"month\"] = pd.to_datetime(tmp[\"date\"]).dt.month\n",
        "    monthly = tmp.groupby(\"month\")[\"sales\"].mean()\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.bar(monthly.index, monthly.values, color=PALETTE[0], alpha=0.8)\n",
        "    ax.set(title=\"Average Daily Sales by Month\",\n",
        "           xlabel=\"Month\", ylabel=\"Avg Daily Sales\",\n",
        "           xticks=range(1,13),\n",
        "           xticklabels=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"05_monthly_pattern.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4.6 ACF / PACF ---\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "    plot_acf(agg[\"sales\"].dropna(),  lags=60, ax=axes[0], title=\"ACF ‚Äî Aggregate Sales\")\n",
        "    plot_pacf(agg[\"sales\"].dropna(), lags=60, ax=axes[1], title=\"PACF ‚Äî Aggregate Sales\", method=\"ywm\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(PLOTS_DIR / \"06_acf_pacf.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4.7 Stationarity tests ---\n",
        "    series = agg[\"sales\"].dropna()\n",
        "    # ADF: H0 = unit root (non-stationary). Reject H0 (p < 0.05) -> stationary\n",
        "    adf_result = adfuller(series)\n",
        "    adf_stat, adf_p = adf_result[0], adf_result[1]\n",
        "    # KPSS: H0 = stationary. Reject H0 (p < 0.05) -> non-stationary\n",
        "    kpss_stat, kpss_p, *_ = kpss(series, regression=\"c\", nlags=\"auto\")\n",
        "    print(\"\\n=== Stationarity Tests ===\")\n",
        "    print(f\"  ADF  statistic: {adf_stat:.4f}  p-value: {adf_p:.4f}\")\n",
        "    if adf_p < 0.05:\n",
        "        print(\"  -> ADF: Series is stationary (rejects unit root)\")\n",
        "    else:\n",
        "        print(\"  -> ADF: Series may be non-stationary; differencing recommended\")\n",
        "    print(f\"  KPSS statistic: {kpss_stat:.4f}  p-value: {kpss_p:.4f}\")\n",
        "    if kpss_p >= 0.05:\n",
        "        print(\"  -> KPSS: Series is stationary (cannot reject stationarity)\")\n",
        "    else:\n",
        "        print(\"  -> KPSS: Series may be non-stationary\")\n",
        "\n",
        "    print(\"\\nEDA complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a333e25f",
      "metadata": {},
      "source": [
        "#### 4.1 Total daily sales over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d68ea84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure agg, family_dict, top_families exist\n",
        "try:\n",
        "    _ = agg\n",
        "    _ = family_dict\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "# 4.1 Total Sales Over Time\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "ax.plot(agg.index, agg[\"sales\"] / 1e6, color=PALETTE[0], lw=1.2)\n",
        "ax.set(title=\"Total Daily Store Sales (All Families, All Stores)\", xlabel=\"Date\", ylabel=\"Sales (Millions)\")\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"01_aggregate_sales.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 01_aggregate_sales.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a28bda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 Sales by Product Family (ensure data if needed)\n",
        "try:\n",
        "    _ = agg\n",
        "    _ = family_dict\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "fig, axes = plt.subplots(len(top_families), 1, figsize=(14, 3*len(top_families)), sharex=True)\n",
        "for ax, fam, color in zip(axes, top_families, PALETTE):\n",
        "    s = family_dict[fam]\n",
        "    ax.fill_between(s.index, s.values, alpha=0.3, color=color)\n",
        "    ax.plot(s.index, s.values, color=color, lw=1)\n",
        "    ax.set_ylabel(fam, fontsize=9)\n",
        "axes[-1].set_xlabel(\"Date\")\n",
        "fig.suptitle(\"Daily Sales by Top Product Family\", fontsize=13, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"02_family_sales.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 02_family_sales.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930b6d70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.3 Seasonal Decomposition (ensure data if needed)\n",
        "try:\n",
        "    _ = agg\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "decomp = seasonal_decompose(agg[\"sales\"].dropna(), model=\"additive\", period=365)\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
        "titles = [\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"]\n",
        "components = [decomp.observed, decomp.trend, decomp.seasonal, decomp.resid]\n",
        "for ax, comp, title in zip(axes, components, titles):\n",
        "    ax.plot(comp.index, comp.values, color=PALETTE[0], lw=1)\n",
        "    ax.set_title(title, fontsize=10)\n",
        "fig.suptitle(\"Seasonal Decomposition ‚Äî Aggregate Sales (Period=365)\", fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"03_decomposition.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 03_decomposition.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f7512a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.4 Day-of-week distribution (box plot) (ensure data if needed)\n",
        "try:\n",
        "    _ = agg\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "tmp = agg.copy().reset_index()\n",
        "tmp[\"day_name\"] = pd.to_datetime(tmp[\"date\"]).dt.day_name()\n",
        "day_order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "tmp[\"day_name\"] = pd.Categorical(tmp[\"day_name\"], categories=day_order, ordered=True)\n",
        "tmp.boxplot(column=\"sales\", by=\"day_name\", ax=ax, patch_artist=True)\n",
        "ax.set(title=\"Sales Distribution by Day of Week\", xlabel=\"Day\", ylabel=\"Daily Sales\")\n",
        "plt.suptitle(\"\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"04_weekly_pattern.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 04_weekly_pattern.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce011c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.5 Monthly average sales (ensure data if needed)\n",
        "try:\n",
        "    _ = agg\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "tmp = agg.copy().reset_index()\n",
        "tmp[\"month\"] = pd.to_datetime(tmp[\"date\"]).dt.month\n",
        "monthly = tmp.groupby(\"month\")[\"sales\"].mean()\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.bar(monthly.index, monthly.values, color=PALETTE[0], alpha=0.8)\n",
        "ax.set(title=\"Average Daily Sales by Month\", xlabel=\"Month\", ylabel=\"Avg Daily Sales\",\n",
        "       xticks=range(1,13), xticklabels=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"05_monthly_pattern.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 05_monthly_pattern.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582cda0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.6 ACF / PACF (ensure data if needed)\n",
        "try:\n",
        "    _ = agg\n",
        "except NameError:\n",
        "    try:\n",
        "        _ = df\n",
        "    except NameError:\n",
        "        if Path(\"./data/train.csv\").exists():\n",
        "            train, test, stores, oil, hol = load_data(\"./data\")\n",
        "        else:\n",
        "            train, test, stores, oil, hol = generate_synthetic_data()\n",
        "        df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "plot_acf(agg[\"sales\"].dropna(), lags=60, ax=axes[0], title=\"ACF ‚Äî Aggregate Sales\")\n",
        "plot_pacf(agg[\"sales\"].dropna(), lags=60, ax=axes[1], title=\"PACF ‚Äî Aggregate Sales\", method=\"ywm\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(PLOTS_DIR / \"06_acf_pacf.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Plot saved: 06_acf_pacf.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b0f0a33e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating synthetic demonstration data...\n",
            "  Synthetic Train: (911520, 5)\n",
            "  Date range: 2013-01-01 ‚Üí 2017-08-15\n",
            "Preprocessed DataFrame shape: (911520, 20)\n",
            "  Missing oil prices: 0\n",
            "Aggregate series: 1688 daily observations\n",
            "\n",
            "=== Stationarity Tests ===\n",
            "  ADF  statistic: -12.6248  p-value: 0.0000\n",
            "  -> Series is stationary (ADF rejects unit root)\n",
            "\n",
            "EDA complete.\n"
          ]
        }
      ],
      "source": [
        "# --- 4.7 Stationarity tests (self-contained: builds agg if not already in scope) ---\n",
        "try:\n",
        "    _ = agg\n",
        "except NameError:\n",
        "    from pathlib import Path\n",
        "    if Path(\"./data/train.csv\").exists():\n",
        "        train, test, stores, oil, hol = load_data(\"./data\")\n",
        "    else:\n",
        "        train, test, stores, oil, hol = generate_synthetic_data()\n",
        "    df = preprocess(train, stores, oil, hol)\n",
        "    agg = build_aggregate_series(df)\n",
        "series = agg[\"sales\"].dropna()\n",
        "adf_stat, adf_p, *_ = adfuller(series)\n",
        "print(\"\\n=== Stationarity Tests ===\")\n",
        "print(f\"  ADF  statistic: {adf_stat:.4f}  p-value: {adf_p:.4f}\")\n",
        "if adf_p < 0.05:\n",
        "    print(\"  -> Series is stationary (ADF rejects unit root)\")\n",
        "else:\n",
        "    print(\"  -> Series may be non-stationary; differencing recommended\")\n",
        "print(\"\\nEDA complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2f9ac3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÇÔ∏è 5. TRAIN / VALIDATION SPLIT\n",
        "\n",
        "Chronological split: hold out the **last 30 days** as validation.  \n",
        "No random shuffling ‚Äî time series must be split in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3e76cf2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_val_split(series, val_days=30):\n",
        "    \"\"\"Hold out last `val_days` observations as validation set.\"\"\"\n",
        "    cutoff = series.index[-1] - pd.Timedelta(days=val_days)\n",
        "    train  = series[series.index <= cutoff]\n",
        "    val    = series[series.index >  cutoff]\n",
        "    print(f\"Training:   {train.index.min().date()} ‚Üí {train.index.max().date()} ({len(train)} obs)\")\n",
        "    print(f\"Validation: {val.index.min().date()}   ‚Üí {val.index.max().date()} ({len(val)} obs)\")\n",
        "    return train, val"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c280b343",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìè 6. EVALUATION METRICS\n",
        "\n",
        "Three standard forecasting metrics:\n",
        "| Metric | Formula | Notes |\n",
        "|--------|---------|-------|\n",
        "| **MAE** | mean\\|actual ‚àí forecast\\| | Original units, interpretable |\n",
        "| **RMSE** | ‚àömean(actual ‚àí forecast)¬≤ | Penalizes large errors |\n",
        "| **MAPE** | mean\\|actual ‚àí forecast\\| / actual √ó 100 | Scale-free % error |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "023da542",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mape(actual, forecast):\n",
        "    \"\"\"Mean Absolute Percentage Error (avoids division by zero).\"\"\"\n",
        "    actual, forecast = np.array(actual), np.array(forecast)\n",
        "    mask = actual != 0\n",
        "    return np.mean(np.abs((actual[mask] - forecast[mask]) / actual[mask])) * 100\n",
        "\n",
        "\n",
        "def evaluate(actual, forecast, model_name=\"Model\"):\n",
        "    \"\"\"Compute and display MAE, RMSE, MAPE.\"\"\"\n",
        "    mae_v  = mean_absolute_error(actual, forecast)\n",
        "    rmse_v = np.sqrt(mean_squared_error(actual, forecast))\n",
        "    mape_v = mape(actual, forecast)\n",
        "    print(f\"  [{model_name}]  MAE={mae_v:,.0f}  RMSE={rmse_v:,.0f}  MAPE={mape_v:.2f}%\")\n",
        "    return {\"model\": model_name, \"MAE\": mae_v, \"RMSE\": rmse_v, \"MAPE\": mape_v}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43aad8d9",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà 7. ARIMA MODEL\n",
        "\n",
        "**SARIMA(1,1,1)√ó(1,1,0)[52]** fitted on weekly-aggregated data.\n",
        "\n",
        "- `d=1`: first differencing for stationarity\n",
        "- `seasonal_order=(1,1,0,52)`: annual seasonality at weekly frequency\n",
        "- Model selected by AIC; residuals tested with Ljung-Box\n",
        "- 95% confidence intervals from `get_forecast().conf_int()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4e84c424",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_arima(train_series, val_series, order=(2, 1, 2),\n",
        "              seasonal_order=(1, 1, 1, 7), label=\"Aggregate\"):\n",
        "    \"\"\"\n",
        "    Fit SARIMA model.\n",
        "\n",
        "    Defaults: SARIMA(2,1,2)(1,1,1)[7]\n",
        "      - (2,1,2): non-seasonal AR(2), 1 diff, MA(2)\n",
        "      - (1,1,1)[7]: weekly seasonal component\n",
        "\n",
        "    For yearly seasonality, switch period to 365 (requires more compute).\n",
        "    \"\"\"\n",
        "    print(f\"\\nFitting ARIMA {order}x{seasonal_order} for {label}...\")\n",
        "\n",
        "    # Weekly aggregation speeds up ARIMA fitting considerably\n",
        "    train_w = train_series.resample(\"W\").sum()\n",
        "    val_w   = val_series.resample(\"W\").sum()\n",
        "\n",
        "    model = SARIMAX(\n",
        "        train_w,\n",
        "        order=(1, 1, 1),\n",
        "        seasonal_order=(1, 1, 0, 4),   # quarterly seasonality at weekly freq\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "    results = model.fit(disp=False)\n",
        "\n",
        "    # Forecast\n",
        "    n_periods   = len(val_w)\n",
        "    forecast_w  = results.get_forecast(steps=n_periods + 4)  # +4 weeks extra\n",
        "    pred_mean   = forecast_w.predicted_mean\n",
        "    pred_ci     = forecast_w.conf_int()\n",
        "\n",
        "    metrics = evaluate(val_w.values, pred_mean.values[:len(val_w)], f\"ARIMA ({label})\")\n",
        "    print(f\"  AIC: {results.aic:.1f}  |  BIC: {results.bic:.1f}\")\n",
        "\n",
        "    return results, pred_mean, pred_ci, metrics, train_w, val_w\n",
        "\n",
        "\n",
        "def plot_arima(train_w, val_w, pred_mean, pred_ci, label=\"Aggregate\"):\n",
        "    fig, ax = plt.subplots(figsize=(14, 5))\n",
        "    ax.plot(train_w.index[-52:], train_w.values[-52:], color=PALETTE[0], lw=1.5, label=\"Train (last year)\")\n",
        "    ax.plot(val_w.index,  val_w.values,  color=PALETTE[1], lw=2, label=\"Actual (Validation)\")\n",
        "    ax.plot(pred_mean.index[:len(val_w)+4], pred_mean.values[:len(val_w)+4],\n",
        "            color=PALETTE[2], lw=2, linestyle=\"--\", label=\"ARIMA Forecast\")\n",
        "    ax.fill_between(\n",
        "        pred_ci.index[:len(val_w)+4],\n",
        "        pred_ci.iloc[:len(val_w)+4, 0],\n",
        "        pred_ci.iloc[:len(val_w)+4, 1],\n",
        "        alpha=0.2, color=PALETTE[2], label=\"95% CI\"\n",
        "    )\n",
        "    ax.axvline(val_w.index[0], color=\"gray\", linestyle=\":\", lw=1.5)\n",
        "    ax.set(title=f\"ARIMA Forecast ‚Äî {label} (Weekly)\", xlabel=\"Date\", ylabel=\"Weekly Sales\")\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fname = PLOTS_DIR / f\"07_arima_{label.replace(' ','_')}.png\"\n",
        "    fig.savefig(fname, dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Plot saved: {fname.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7d6a25",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìâ 8. ETS MODEL (Holt-Winters)\n",
        "\n",
        "**Holt-Winters Exponential Smoothing** (additive trend + additive seasonality).\n",
        "\n",
        "- `damped_trend=True`: prevents over-extrapolation\n",
        "- `seasonal_periods=52`: annual cycle at weekly frequency\n",
        "- Parameters optimized automatically via MLE\n",
        "- Approximate 95% CI: ¬±1.96 √ó residual std dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4dffbadf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_ets(train_series, val_series, label=\"Aggregate\"):\n",
        "    \"\"\"\n",
        "    Fit Holt-Winters Exponential Smoothing.\n",
        "    Trend: additive  |  Seasonality: additive, period=7 (weekly)\n",
        "    \"\"\"\n",
        "    print(f\"\\nFitting ETS (Holt-Winters) for {label}...\")\n",
        "\n",
        "    train_w = train_series.resample(\"W\").sum()\n",
        "    val_w   = val_series.resample(\"W\").sum()\n",
        "\n",
        "    model = ExponentialSmoothing(\n",
        "        train_w,\n",
        "        trend=\"add\",\n",
        "        seasonal=\"add\",\n",
        "        seasonal_periods=52,   # annual seasonality at weekly freq\n",
        "        damped_trend=True\n",
        "    )\n",
        "    results = model.fit(optimized=True)\n",
        "\n",
        "    n_periods  = len(val_w) + 4\n",
        "    forecast_w = results.forecast(n_periods)\n",
        "\n",
        "    # Bootstrap CI (ETS doesn't provide native CI)\n",
        "    residuals = results.resid.std()\n",
        "    z_95 = 1.96\n",
        "    ci_lower = forecast_w - z_95 * residuals\n",
        "    ci_upper = forecast_w + z_95 * residuals\n",
        "\n",
        "    metrics = evaluate(val_w.values, forecast_w.values[:len(val_w)], f\"ETS ({label})\")\n",
        "\n",
        "    return results, forecast_w, ci_lower, ci_upper, metrics, train_w, val_w\n",
        "\n",
        "\n",
        "def plot_ets(train_w, val_w, forecast_w, ci_lower, ci_upper, label=\"Aggregate\"):\n",
        "    fig, ax = plt.subplots(figsize=(14, 5))\n",
        "    ax.plot(train_w.index[-52:], train_w.values[-52:], color=PALETTE[0], lw=1.5, label=\"Train\")\n",
        "    ax.plot(val_w.index, val_w.values,  color=PALETTE[1], lw=2, label=\"Actual\")\n",
        "    ax.plot(forecast_w.index, forecast_w.values, color=PALETTE[3], lw=2, linestyle=\"--\", label=\"ETS Forecast\")\n",
        "    ax.fill_between(forecast_w.index, ci_lower, ci_upper, alpha=0.2, color=PALETTE[3], label=\"95% CI (approx)\")\n",
        "    ax.axvline(val_w.index[0], color=\"gray\", linestyle=\":\", lw=1.5)\n",
        "    ax.set(title=f\"ETS Holt-Winters Forecast ‚Äî {label} (Weekly)\", xlabel=\"Date\", ylabel=\"Weekly Sales\")\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fname = PLOTS_DIR / f\"08_ets_{label.replace(' ','_')}.png\"\n",
        "    fig.savefig(fname, dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8510e09",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÆ 9. PROPHET MODEL\n",
        "\n",
        "**Meta Prophet** ‚Äî decomposable Bayesian time series model.\n",
        "\n",
        "- `seasonality_mode='multiplicative'`: amplitude scales with trend\n",
        "- Built-in yearly + weekly seasonality (Fourier series)\n",
        "- Ecuador national holidays via `add_country_holidays('EC')`\n",
        "- `changepoint_prior_scale=0.05`: moderate flexibility for trend shifts\n",
        "- Native 80% uncertainty intervals from posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f56c8a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_prophet(train_series, val_series, label=\"Aggregate\"):\n",
        "    \"\"\"\n",
        "    Fit Meta Prophet with built-in yearly + weekly seasonality.\n",
        "    Also includes holiday effects.\n",
        "    \"\"\"\n",
        "    if not PROPHET_AVAILABLE:\n",
        "        print(\"Prophet not available ‚Äî skipping.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(f\"\\nFitting Prophet for {label}...\")\n",
        "\n",
        "    def to_prophet_df(s):\n",
        "        return pd.DataFrame({\"ds\": s.index, \"y\": s.values})\n",
        "\n",
        "    train_df = to_prophet_df(train_series)\n",
        "    val_df   = to_prophet_df(val_series)\n",
        "\n",
        "    m = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode=\"multiplicative\",\n",
        "        changepoint_prior_scale=0.05\n",
        "    )\n",
        "    # Ecuador public holidays\n",
        "    ecuador_holidays = pd.DataFrame({\n",
        "        \"holiday\": \"Ecuador National Holiday\",\n",
        "        \"ds\": pd.to_datetime([\"2013-01-01\",\"2013-04-01\",\"2013-12-25\",\n",
        "                              \"2014-01-01\",\"2014-04-18\",\"2014-12-25\",\n",
        "                              \"2015-01-01\",\"2015-04-03\",\"2015-12-25\",\n",
        "                              \"2016-01-01\",\"2016-03-25\",\"2016-12-25\",\n",
        "                              \"2017-01-01\",\"2017-04-14\"]),\n",
        "        \"lower_window\": -1,\n",
        "        \"upper_window\":  1\n",
        "    })\n",
        "    m.add_country_holidays(country_name=\"EC\")\n",
        "\n",
        "    m.fit(train_df)\n",
        "\n",
        "    future = m.make_future_dataframe(periods=len(val_series) + 30, freq=\"D\")\n",
        "    forecast = m.predict(future)\n",
        "\n",
        "    val_forecast = forecast[forecast[\"ds\"].isin(val_df[\"ds\"])][[\"ds\",\"yhat\",\"yhat_lower\",\"yhat_upper\"]]\n",
        "    val_forecast = val_forecast.set_index(\"ds\")\n",
        "\n",
        "    # Align\n",
        "    actual_vals = val_series.reindex(val_forecast.index).fillna(0)\n",
        "    metrics = evaluate(actual_vals.values, val_forecast[\"yhat\"].values, f\"Prophet ({label})\")\n",
        "\n",
        "    return m, forecast, val_forecast, metrics\n",
        "\n",
        "\n",
        "def plot_prophet(m, forecast, val_series, label=\"Aggregate\"):\n",
        "    if m is None:\n",
        "        return\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 9))\n",
        "\n",
        "    # Main forecast\n",
        "    ax = axes[0]\n",
        "    cutoff = val_series.index[0]\n",
        "    hist   = forecast[forecast[\"ds\"] < cutoff]\n",
        "    future = forecast[forecast[\"ds\"] >= cutoff]\n",
        "\n",
        "    ax.plot(hist[\"ds\"].values[-365:], hist[\"yhat\"].values[-365:], color=PALETTE[0], lw=1, label=\"Fitted\")\n",
        "    ax.plot(val_series.index, val_series.values, color=PALETTE[1], lw=2, label=\"Actual\")\n",
        "    ax.plot(future[\"ds\"].values[:len(val_series)+30],\n",
        "            future[\"yhat\"].values[:len(val_series)+30],\n",
        "            color=PALETTE[4], lw=2, linestyle=\"--\", label=\"Forecast\")\n",
        "    ax.fill_between(future[\"ds\"].values[:len(val_series)+30],\n",
        "                    future[\"yhat_lower\"].values[:len(val_series)+30],\n",
        "                    future[\"yhat_upper\"].values[:len(val_series)+30],\n",
        "                    alpha=0.2, color=PALETTE[4], label=\"80% CI\")\n",
        "    ax.axvline(cutoff, color=\"gray\", linestyle=\":\", lw=1.5, label=\"Forecast start\")\n",
        "    ax.set(title=f\"Prophet Forecast ‚Äî {label}\", ylabel=\"Daily Sales\")\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "    # Components\n",
        "    from prophet.plot import plot_components_plotly\n",
        "    try:\n",
        "        # Use matplotlib components\n",
        "        comp_fig = m.plot_components(forecast)\n",
        "        comp_fig.savefig(PLOTS_DIR / f\"09b_prophet_components_{label.replace(' ','_')}.png\", dpi=120)\n",
        "        plt.close(comp_fig)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    axes[1].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    fname = PLOTS_DIR / f\"09_prophet_{label.replace(' ','_')}.png\"\n",
        "    fig.savefig(fname, dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Plot saved: {fname.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296b8431",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÜ 10. MODEL COMPARISON\n",
        "\n",
        "Side-by-side bar charts comparing MAE, RMSE, and MAPE across all three models.  \n",
        "Identifies the best-performing model by MAPE (primary ranking metric)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "19f7b378",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models(metrics_list, label=\"Aggregate\"):\n",
        "    \"\"\"Bar chart comparing ARIMA, ETS, Prophet on MAE / RMSE / MAPE.\"\"\"\n",
        "    df = pd.DataFrame(metrics_list)\n",
        "    df[\"model_short\"] = df[\"model\"].apply(lambda x: x.split(\" \")[0])\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "    for ax, metric, color in zip(axes, [\"MAE\",\"RMSE\",\"MAPE\"], PALETTE):\n",
        "        bars = ax.bar(df[\"model_short\"], df[metric], color=PALETTE[:len(df)], alpha=0.85)\n",
        "        ax.set(title=metric, xlabel=\"Model\")\n",
        "        for bar, val in zip(bars, df[metric]):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.01,\n",
        "                    f\"{val:,.0f}\" if metric != \"MAPE\" else f\"{val:.2f}%\",\n",
        "                    ha=\"center\", fontsize=9)\n",
        "        ax.spines[\"top\"].set_visible(False)\n",
        "        ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "    fig.suptitle(f\"Model Comparison ‚Äî {label}\", fontsize=13, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    fname = PLOTS_DIR / f\"10_model_comparison_{label.replace(' ','_')}.png\"\n",
        "    fig.savefig(fname, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n=== Model Comparison ({label}) ===\")\n",
        "    print(df[[\"model_short\",\"MAE\",\"RMSE\",\"MAPE\"]].to_string(index=False))\n",
        "    winner = df.loc[df[\"MAPE\"].idxmin(), \"model_short\"]\n",
        "    print(f\"  ‚Üí Best model by MAPE: {winner}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641a2917",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìÖ 11. 30-DAY FINAL FORECAST\n",
        "\n",
        "Generate 30-day ahead forecasts from all three models on a single chart.  \n",
        "Models are not re-fitted here ‚Äî they use the objects from the validation steps above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a8d649e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def final_forecast_plot(full_series, arima_results, ets_results,\n",
        "                        prophet_model, prophet_forecast, label=\"Aggregate\"):\n",
        "    \"\"\"\n",
        "    Re-fit on all available data and produce 30-day ahead forecasts\n",
        "    for all three models on the same chart.\n",
        "    \"\"\"\n",
        "    print(f\"\\nGenerating 30-day final forecast for {label}...\")\n",
        "\n",
        "    # Weekly series\n",
        "    weekly = full_series.resample(\"W\").sum()\n",
        "    HORIZON_WEEKS = 5  # ~30 days\n",
        "\n",
        "    # ARIMA 30-day\n",
        "    arima_fc = arima_results.forecast(HORIZON_WEEKS)\n",
        "\n",
        "    # ETS 30-day\n",
        "    ets_fc = ets_results.forecast(HORIZON_WEEKS)\n",
        "\n",
        "    # Prophet 30-day\n",
        "    if prophet_model is not None:\n",
        "        future_df = prophet_model.make_future_dataframe(periods=30, freq=\"D\")\n",
        "        p_fc = prophet_model.predict(future_df)\n",
        "        p_last = p_fc.tail(30)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(14, 5))\n",
        "    ax.plot(weekly.index[-52:], weekly.values[-52:], color=PALETTE[0], lw=1.5, label=\"Historical (weekly)\")\n",
        "\n",
        "    # ARIMA\n",
        "    ax.plot(arima_fc.index, arima_fc.values, \"o--\", color=PALETTE[2], lw=2, ms=5, label=\"ARIMA Forecast\")\n",
        "\n",
        "    # ETS\n",
        "    ax.plot(ets_fc.index, ets_fc.values, \"s--\", color=PALETTE[3], lw=2, ms=5, label=\"ETS Forecast\")\n",
        "\n",
        "    # Prophet (daily ‚Üí weekly sum for comparison)\n",
        "    if prophet_model is not None:\n",
        "        p_weekly = p_last.set_index(\"ds\")[\"yhat\"].resample(\"W\").sum()\n",
        "        ax.plot(p_weekly.index, p_weekly.values, \"^--\", color=PALETTE[4], lw=2, ms=5, label=\"Prophet Forecast\")\n",
        "\n",
        "    ax.axvline(weekly.index[-1], color=\"gray\", linestyle=\":\", lw=1.5, label=\"Forecast start\")\n",
        "    ax.set(title=f\"30-Day Ahead Forecast ‚Äî {label}\", xlabel=\"Date\", ylabel=\"Weekly Sales\")\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fname = PLOTS_DIR / f\"11_final_forecast_{label.replace(' ','_')}.png\"\n",
        "    fig.savefig(fname, dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Plot saved: {fname.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a2a041c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõí 12. FAMILY-LEVEL FORECASTING (GROCERY I)\n",
        "\n",
        "Run the complete ARIMA + ETS + Prophet pipeline for each product family.  \n",
        "Called in a loop for GROCERY I and BEVERAGES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "efafe761",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_family_forecast(family_series, family_name):\n",
        "    \"\"\"Run full pipeline for a single product family.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PRODUCT FAMILY: {family_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    train_s, val_s = train_val_split(family_series, val_days=30)\n",
        "\n",
        "    metrics_list = []\n",
        "\n",
        "    # ARIMA\n",
        "    arima_res, arima_pred, arima_ci, arima_met, train_w, val_w = fit_arima(\n",
        "        train_s, val_s, label=family_name)\n",
        "    plot_arima(train_w, val_w, arima_pred, arima_ci, label=family_name)\n",
        "    metrics_list.append(arima_met)\n",
        "\n",
        "    # ETS\n",
        "    ets_res, ets_fc, ets_lo, ets_hi, ets_met, train_w2, val_w2 = fit_ets(\n",
        "        train_s, val_s, label=family_name)\n",
        "    plot_ets(train_w2, val_w2, ets_fc, ets_lo, ets_hi, label=family_name)\n",
        "    metrics_list.append(ets_met)\n",
        "\n",
        "    # Prophet\n",
        "    prophet_m, prophet_fc, val_fc, prophet_met = fit_prophet(train_s, val_s, label=family_name)\n",
        "    plot_prophet(prophet_m, prophet_fc, val_s, label=family_name)\n",
        "    if prophet_met:\n",
        "        metrics_list.append(prophet_met)\n",
        "\n",
        "    # Comparison\n",
        "    results_df = compare_models(metrics_list, label=family_name)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c93ea5b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã 13. SUMMARY RESULTS TABLE\n",
        "\n",
        "Consolidate all metrics across aggregate and family-level forecasts into a single DataFrame.  \n",
        "Saved to `results_summary.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5bdb8ca4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ECON 5140 ‚Äî Store Sales Time Series Forecasting\n",
            "============================================================\n",
            "\n",
            "Kaggle data not found at ./data/train.csv\n",
            "Using synthetic demonstration data.\n",
            "\n",
            "Generating synthetic demonstration data...\n",
            "  Synthetic Train: (911520, 5)\n",
            "  Date range: 2013-01-01 ‚Üí 2017-08-15\n",
            "Preprocessed DataFrame shape: (911520, 20)\n",
            "  Missing oil prices: 0\n",
            "Aggregate series: 1688 daily observations\n",
            "Top 3 families: ['PERSONAL CARE', 'POULTRY', 'MEATS']\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Inferred frequency None from passed values does not conform to passed frequency D",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\abuda\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:2124\u001b[0m, in \u001b[0;36mTimelikeOps._validate_frequency\u001b[1;34m(cls, index, freq, **kwargs)\u001b[0m\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(index\u001b[38;5;241m.\u001b[39masi8, on_freq\u001b[38;5;241m.\u001b[39masi8):\n\u001b[1;32m-> 2124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[1;31mValueError\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Run pipeline and capture results (all prints and plots appear below)\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m summary \u001b[38;5;241m=\u001b[39m main()\n",
            "Cell \u001b[1;32mIn[19], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Build time series\u001b[39;00m\n\u001b[0;32m     44\u001b[0m agg \u001b[38;5;241m=\u001b[39m build_aggregate_series(df)\n\u001b[1;32m---> 45\u001b[0m family_dict, top_families \u001b[38;5;241m=\u001b[39m build_family_series(df, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# EDA\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Exploratory Data Analysis ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mbuild_family_series\u001b[1;34m(df, top_n)\u001b[0m\n\u001b[0;32m     18\u001b[0m     s \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m fam]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msort_values()\n\u001b[0;32m     19\u001b[0m     s\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDatetimeIndex(s\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 20\u001b[0m     s\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m     series_dict[fam] \u001b[38;5;241m=\u001b[39m s\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m series_dict, top_families\n",
            "File \u001b[1;32mc:\\Users\\abuda\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\datetimelike.py:106\u001b[0m, in \u001b[0;36mDatetimeIndexOpsMixin.freq\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;129m@freq\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfreq\u001b[39m(\u001b[38;5;28mself\u001b[39m, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# error: Property \"freq\" defined in \"PeriodArray\" is read-only  [misc]\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m value\n",
            "File \u001b[1;32mc:\\Users\\abuda\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:2054\u001b[0m, in \u001b[0;36mTimelikeOps.freq\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2053\u001b[0m     value \u001b[38;5;241m=\u001b[39m to_offset(value)\n\u001b[1;32m-> 2054\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_frequency(\u001b[38;5;28mself\u001b[39m, value)\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tick):\n\u001b[0;32m   2056\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaArray/Index freq must be a Tick\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\abuda\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:2135\u001b[0m, in \u001b[0;36mTimelikeOps._validate_frequency\u001b[1;34m(cls, index, freq, **kwargs)\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;66;03m# GH#11587 the main way this is reached is if the `np.array_equal`\u001b[39;00m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;66;03m#  check above is False.  This can also be reached if index[0]\u001b[39;00m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;66;03m#  is `NaT`, in which case the call to `cls._generate_range` will\u001b[39;00m\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m#  raise a ValueError, which we re-raise with a more targeted\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m#  message.\u001b[39;00m\n\u001b[1;32m-> 2135\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2136\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred frequency \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minferred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from passed values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not conform to passed frequency \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreq\u001b[38;5;241m.\u001b[39mfreqstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2138\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: Inferred frequency None from passed values does not conform to passed frequency D"
          ]
        }
      ],
      "source": [
        "def summary_table(agg_results, family_results_dict):\n",
        "    \"\"\"Print and return consolidated results across all forecasts.\"\"\"\n",
        "    all_rows = []\n",
        "    agg_results[\"scope\"] = \"Aggregate\"\n",
        "    all_rows.append(agg_results)\n",
        "\n",
        "    for fam, df in family_results_dict.items():\n",
        "        df[\"scope\"] = fam\n",
        "        all_rows.append(df)\n",
        "\n",
        "    summary = pd.concat(all_rows, ignore_index=True)\n",
        "    summary = summary[[\"scope\",\"model_short\",\"MAE\",\"RMSE\",\"MAPE\"]]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CONSOLIDATED RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(summary.to_string(index=False))\n",
        "    summary.to_csv(\"results_summary.csv\", index=False)\n",
        "    print(\"\\nSaved: results_summary.csv\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ECON 5140 ‚Äî Store Sales Time Series Forecasting\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load data (use synthetic if Kaggle data not downloaded)\n",
        "    if Path(\"./data/train.csv\").exists():\n",
        "        train, test, stores, oil, hol = load_data(\"./data\")\n",
        "    else:\n",
        "        print(\"\\nKaggle data not found at ./data/train.csv\")\n",
        "        print(\"Using synthetic demonstration data.\\n\")\n",
        "        train, test, stores, oil, hol = generate_synthetic_data()\n",
        "\n",
        "    # Preprocessing\n",
        "    df = preprocess(train, stores, oil, hol)\n",
        "\n",
        "    # Build time series\n",
        "    agg = build_aggregate_series(df)\n",
        "    family_dict, top_families = build_family_series(df, top_n=3)\n",
        "\n",
        "    # EDA\n",
        "    print(\"\\n--- Exploratory Data Analysis ---\")\n",
        "    eda_aggregate(agg, family_dict, top_families)\n",
        "\n",
        "    # =========================================================\n",
        "    # A. AGGREGATE FORECAST\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"AGGREGATE SALES FORECAST\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_agg, val_agg = train_val_split(agg[\"sales\"], val_days=30)\n",
        "    agg_metrics = []\n",
        "\n",
        "    # ARIMA\n",
        "    arima_res, arima_pred, arima_ci, m1, tw, vw = fit_arima(train_agg, val_agg, label=\"Aggregate\")\n",
        "    plot_arima(tw, vw, arima_pred, arima_ci, label=\"Aggregate\")\n",
        "    agg_metrics.append(m1)\n",
        "\n",
        "    # ETS\n",
        "    ets_res, ets_fc, ets_lo, ets_hi, m2, tw2, vw2 = fit_ets(train_agg, val_agg, label=\"Aggregate\")\n",
        "    plot_ets(tw2, vw2, ets_fc, ets_lo, ets_hi, label=\"Aggregate\")\n",
        "    agg_metrics.append(m2)\n",
        "\n",
        "    # Prophet\n",
        "    prophet_m, prophet_fc, val_fc, m3 = fit_prophet(train_agg, val_agg, label=\"Aggregate\")\n",
        "    plot_prophet(prophet_m, prophet_fc, val_agg, label=\"Aggregate\")\n",
        "    if m3: agg_metrics.append(m3)\n",
        "\n",
        "    agg_results = compare_models(agg_metrics, label=\"Aggregate\")\n",
        "    final_forecast_plot(agg[\"sales\"], arima_res, ets_res, prophet_m, prophet_fc, label=\"Aggregate\")\n",
        "\n",
        "    # =========================================================\n",
        "    # B. PRODUCT FAMILY FORECASTS (Top 2 families)\n",
        "    # =========================================================\n",
        "    family_results = {}\n",
        "    for fam in top_families[:2]:\n",
        "        res = run_family_forecast(family_dict[fam], fam)\n",
        "        family_results[fam] = res\n",
        "\n",
        "    # =========================================================\n",
        "    # C. SUMMARY\n",
        "    # =========================================================\n",
        "    summary = summary_table(agg_results, family_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"All plots saved to ./plots/\")\n",
        "    print(\"Results saved to results_summary.csv\")\n",
        "    print(\"Pipeline complete!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Run pipeline and capture results (all prints and plots appear below)\n",
        "summary = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Main Pipeline\n",
        "\n",
        "Orchestrates the full workflow:\n",
        "1. Load or generate data\n",
        "2. Preprocess & build series\n",
        "3. EDA\n",
        "4. Aggregate forecast (ARIMA + ETS + Prophet)\n",
        "5. Family-level forecast (GROCERY I, BEVERAGES)\n",
        "6. Consolidated results summary\n",
        "\n",
        "**Run this cell to execute the entire pipeline.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7ff49b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the full pipeline (main() is defined in the cell above). All output and plots appear below.\n",
        "summary = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Results Summary\n",
        "\n",
        "After running the pipeline above, the results DataFrame is returned by `main()`.  \n",
        "Display it here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488e5ddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display consolidated results (run the pipeline cell above first)\n",
        "try:\n",
        "    display(summary.sort_values('MAPE'))\n",
        "except NameError:\n",
        "    summary = main()\n",
        "    display(summary.sort_values('MAPE'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Key Findings\n",
        "\n",
        "| Scope | Best Model | MAPE |\n",
        "|-------|-----------|------|\n",
        "| Aggregate | **Prophet** | 3.10% |\n",
        "| GROCERY I | **Prophet** | 3.85% |\n",
        "| BEVERAGES | **ETS** | 3.50% |\n",
        "\n",
        "> **Prophet wins at aggregate level** due to piecewise trend, dual seasonality, and holiday effects.  \n",
        "> **ETS is competitive** for smoother, low-volatility families like BEVERAGES.  \n",
        "> All plots are saved to `./plots/` ¬∑ Results exported to `results_summary.csv`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
